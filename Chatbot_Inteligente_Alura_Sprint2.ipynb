{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9A2oSLDivrbN",
        "qQEIdACV2IE4",
        "kDRIGcy62JuE",
        "_RnqKO9ChOpY",
        "IVJ6UmQV3n4T"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaudraccoTomas/ChatBootInteligente_Alura/blob/main/Chatbot_Inteligente_Alura_Sprint2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configurar ambiente"
      ],
      "metadata": {
        "id": "9A2oSLDivrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_md\n",
        "!pip install jellyfish\n",
        "!pip install transformers\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbzs7ekFNpxU",
        "outputId": "41ec27c6-9bfc-42a2-c60d-4183ba5706b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-09 12:10:16.110121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-09 12:10:18.230401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.6.0/es_core_news_md-3.6.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jellyfish\n",
            "Successfully installed jellyfish-1.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=0022da69c8681e81a16f734b97a20527dc78ad8e334ec8277095280a3350c885\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UD0OObFGtZVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568c2268-160f-44fa-8f49-09b157839a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Instalando bibliotecas\n",
        "import pandas as pd\n",
        "import re, os, random, pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import jellyfish\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from docx import Document\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Definiendo variables del proyecto:\n",
        "nlp = spacy.load('es_core_news_md')\n",
        "\n",
        "#Conectando al Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/MyDrive/Chatbot'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importar verbos"
      ],
      "metadata": {
        "id": "qQEIdACV2IE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la lista_verbos:\n",
        "pickle_file = open(\"/content/drive/MyDrive/Chatbot/verbos/lista_verbos.pickle\", \"rb\")\n",
        "lista_verbos = pickle.load(pickle_file)\n",
        "\n",
        "# Importar el diccionario:\n",
        "pickle_file = open(\"/content/drive/MyDrive/Chatbot/verbos/verbos_irregulares.pickle\", \"rb\")\n",
        "verbos_irregulares = pickle.load(pickle_file)"
      ],
      "metadata": {
        "id": "8DDVG_F22IvU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tratamiento de datos"
      ],
      "metadata": {
        "id": "kDRIGcy62JuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33l50I4Ehwiz",
        "outputId": "01c5deda-9cd4-431f-e36e-bc76b18e7cd7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "from nltk.tokenize import WhitespaceTokenizer"
      ],
      "metadata": {
        "id": "6B7TGphBhqjE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para encontrar la raiz de las palabras\n",
        "def raiz(palabra):\n",
        "  \"\"\"\n",
        "  Funcion para comparar similitud entre palabras\\n\n",
        "  recibe(palabraAComparar, listaDePalabrasdeDondeComparar)\\n\n",
        "  return (palabra elegida,\\n\n",
        "          porcentaje de similitud,\\n\n",
        "          palabra mas cercana)\n",
        "  \"\"\"\n",
        "  similar = 0\n",
        "  contador = 0\n",
        "  verbo_similar = \"\"\n",
        "  for verbo in lista_verbos:\n",
        "    contador += 1\n",
        "    radio = jellyfish.jaro_winkler_similarity(palabra, verbo)\n",
        "    if radio > similar:\n",
        "      similar = radio\n",
        "      verbo_similar = verbo\n",
        "  if similar < 0.93:\n",
        "    verbo_final = palabra\n",
        "  else:\n",
        "    verbo_final = verbo_similar\n",
        "  return verbo_final\n",
        "\n",
        "def tratamiento_texto(texto):\n",
        "  \"\"\"\n",
        "  Se ingresa un texto y se devuelve texto:\\n\n",
        "  · En minuscula\\n\n",
        "  · Sin acentos\\n\n",
        "  · Sin signos de puntuacion\n",
        "  \"\"\"\n",
        "  lista_palabras = []\n",
        "  #texto = texto.lower()\n",
        "  texto = unidecode(texto)\n",
        "  espaciado = WhitespaceTokenizer()\n",
        "  texto_espacios = espaciado.tokenize(texto)\n",
        "\n",
        "  for palabra in texto_espacios:\n",
        "    palabra_limpia = \"\".join([letra for letra in palabra if letra.isalnum()])\n",
        "    lista_palabras.append(palabra_limpia)\n",
        "\n",
        "  texto_limpio = \" \".join([i for i in lista_palabras])\n",
        "  return texto_limpio\n",
        "\n",
        "#Función para reemplazar el final de una palabra por 'r'\n",
        "def reemplazar_terminacion(palabra):\n",
        "  \"\"\"\n",
        "  Intenta acercar los verbos conjugados lo mas posible a su forma en infinitivo\n",
        "  · Remplaza las terminaciones \"ar\", \"es\", \"me\", \"as\", \"te\" por \"r\" y la terminacion \"ste\" por \"ar\"\n",
        "  \"\"\"\n",
        "  lista_palabras = []\n",
        "  espaciado = WhitespaceTokenizer()\n",
        "  texto_espacios = espaciado.tokenize(palabra)\n",
        "\n",
        "  for texto in texto_espacios:\n",
        "    if texto[-3:] == \"ste\" and texto != \"este\":\n",
        "      texto = texto[:-3] + \"ar\"\n",
        "    elif (texto[-2:] == \"es\" or texto[-2:] == \"me\" or texto[-2:] == \"as\" or texto[-2:] == \"te\") and len(texto) > 3:\n",
        "      texto = texto[:-2] + \"r\"\n",
        "    lista_palabras.append(texto)\n",
        "\n",
        "  texto_limpio = \" \".join([i for i in lista_palabras])\n",
        "  return texto_limpio.split()[0]\n",
        "\n",
        "#Función para adicionar o eliminar tokens\n",
        "def revisar_tokens(texto, tokens):\n",
        "    # Si la lista de tokens está vacía, vamos a incluir tokens especiales\n",
        "    if len(tokens) == 0:\n",
        "        # Si encuentras alguna palabra compuesta especial en el texto, agrega su token\n",
        "        if 'cientifico de datos' in texto or 'data scientist' in texto:\n",
        "            tokens.append('datascientist')\n",
        "        if 'ciencia de datos' in texto or 'data science' in texto:\n",
        "            tokens.append('datascience')\n",
        "        if 'elprofealejo' in texto or 'el profe alejo' in texto or 'profe alejo' in texto or 'profealejo' in texto:\n",
        "            tokens.append('elprofealejo')\n",
        "    else:\n",
        "        # Tokens a eliminar si se encuentran en el texto\n",
        "        elementos_a_eliminar = [\"cual\", \"que\", \"quien\", \"cuanto\", \"cuando\", \"como\"]\n",
        "        if 'hablame' in texto and 'hablar' in tokens:\n",
        "            elementos_a_eliminar.append('hablar')\n",
        "        elif 'cuentame' in texto and 'contar' in tokens:\n",
        "            elementos_a_eliminar.append('contar')\n",
        "        elif 'hago' in texto and 'hacer' in tokens:\n",
        "            elementos_a_eliminar.append('hacer')\n",
        "        elif 'entiendes' in texto and 'entender' in tokens:\n",
        "            elementos_a_eliminar.append('entender')\n",
        "        elif 'sabes' in texto and 'saber' in tokens:\n",
        "            elementos_a_eliminar.append('saber')\n",
        "\n",
        "        # Elimina los elementos de la lista de tokens si están en la lista de elementos a eliminar\n",
        "        tokens = [token for token in tokens if token not in elementos_a_eliminar]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "#Función para devolver los tokens normalizados del texto\n",
        "def normalizar(texto):\n",
        "  tokens=[]\n",
        "  palabras_normalizadas = []\n",
        "  tokens=revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "  doc = nlp(texto)\n",
        "  for t in doc:\n",
        "    if t.is_alpha:\n",
        "      palabra = t.text.lower()\n",
        "      # Reemplazar verbos irregulares\n",
        "      lemma = verbos_irregulares.get(palabra, t.lemma_)\n",
        "      # Eliminar caracteres especiales\n",
        "      lemma = re.sub(r'[^\\w\\s+\\-*/]', '', lemma)\n",
        "      # Agregar la palabra normalizada a la lista\n",
        "      palabras_normalizadas.append(lemma)\n",
        "      if t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','ADJ','ADV','NUM') or lemma in lista_verbos:\n",
        "        if t.pos_=='VERB':\n",
        "          lemma = reemplazar_terminacion(lemma)\n",
        "          tokens.append(raiz(tratamiento_texto(lemma)))\n",
        "        else:\n",
        "          tokens.append(tratamiento_texto(lemma))\n",
        "\n",
        "  tokens = list(dict.fromkeys(tokens))\n",
        "  tokens = list(filter(None, tokens))\n",
        "  tokens = revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "N-__6F0I2MjG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cargar bases de documentos"
      ],
      "metadata": {
        "id": "_RnqKO9ChOpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases de dialogo fluído\n",
        "txt_folder_path = \"/content/drive/MyDrive/Chatbot/dialogos\"\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "lista_dialogos, lista_dialogos_respuesta, lista_tipo_dialogo = [],[],[]\n",
        "for idx in range(len(lista_documentos)):\n",
        "  f=open(txt_folder_path+'/'+lista_documentos[idx], 'r', encoding='utf-8', errors='ignore')\n",
        "  contador = 0\n",
        "  for linea in f.read().split('\\n'):\n",
        "    if contador % 2 == 0:\n",
        "      #linea = normalizacion(linea)\n",
        "      linea = re.sub(r\"[^\\w\\s+\\-*/]\", '', linea)\n",
        "      linea = re.sub(r\"[.*^\\n]\", '', linea)\n",
        "      lista_dialogos.append(linea)\n",
        "      lista_tipo_dialogo.append(lista_documentos[idx][:-4])\n",
        "    else:\n",
        "      linea = re.sub(r\"[^\\w\\s+\\-*/]\", '', linea)\n",
        "      linea = re.sub(r\"[.*^\\n]\", '', linea)\n",
        "      lista_dialogos_respuesta.append(linea)\n",
        "    contador += 1\n",
        "\n",
        "#Creando Dataframe de diálogos\n",
        "datos = {'dialogo':lista_dialogos,'respuesta':lista_dialogos_respuesta,'tipo':lista_tipo_dialogo,'interseccion':0,'jaro_winkler':0,'probabilidad':0}\n",
        "df_dialogo = pd.DataFrame(datos)\n",
        "df_dialogo = df_dialogo.drop_duplicates(keep='first')\n",
        "df_dialogo.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "1894SyvPhQJ0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dialogo"
      ],
      "metadata": {
        "id": "E6az5OLqYoyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases csv\n",
        "txt_folder_path = '/content/drive/MyDrive/Chatbot/documentos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".csv\")]\n",
        "documento_csv = 'tu código aqui'\n",
        "\n",
        "#Importando bases csv\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".csv\")]\n",
        "documento_csv = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[i], \"r\", encoding=\"utf-8\") as csv_txt:\n",
        "    csv_text = csv.reader(csv_txt)\n",
        "    for fila in csv_text:\n",
        "      print(fila[-1])\n",
        "      if fila[-1]!='frase':\n",
        "        documento_csv += fila[-1]\n",
        "print(documento_csv)"
      ],
      "metadata": {
        "id": "5MpTno_7XRfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases docx\n",
        "\n",
        "txt_folder_path = '/content/drive/MyDrive/Chatbot/documentos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".docx\")]\n",
        "documento_docx = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  for t in Document(txt_folder_path+'/'+lista_documentos[i]).paragraphs:\n",
        "    documento_docx += t.text.replace('*','\\n\\n*')+ \"\\n\"\n",
        "print(documento_docx)"
      ],
      "metadata": {
        "id": "tvJIjYQTa1Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases txt\n",
        "txt_folder_path = '/content/drive/MyDrive/Chatbot/documentos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "documento_txt = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[i], \"r\", encoding=\"utf-8\") as txt:\n",
        "    txt_new = txt.read()\n",
        "    for i in txt_new:\n",
        "      documento_txt += i\n",
        "print(documento_txt)"
      ],
      "metadata": {
        "id": "y8krW04Fb9cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documento = documento_csv + documento_txt + documento_docx\n",
        "lista_frases = nltk.sent_tokenize(documento,'spanish')\n",
        "lista_frases_normalizadas = [' '.join(normalizar(x)) for x in lista_frases]"
      ],
      "metadata": {
        "id": "5rkMKiFqZhP6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Buscar respuesta del Chatbot"
      ],
      "metadata": {
        "id": "IVJ6UmQV3n4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interseccion(usuario, pregunta_dialogo):\n",
        "  interseccion = 0\n",
        "  largo_palabra = 0\n",
        "  dialogo = WhitespaceTokenizer().tokenize(pregunta_dialogo)\n",
        "  for palabra_usuario in usuario:\n",
        "    largo_palabra += 1\n",
        "    for palabra_dialogo in dialogo:\n",
        "      if palabra_usuario == palabra_dialogo:\n",
        "        interseccion += 1\n",
        "  porcentaje = interseccion / largo_palabra\n",
        "  return round(porcentaje, 2)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def similarity(pregunta_usuario, pregunta_dialogo):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer.fit_transform(df_dialogo[\"dialogo\"])\n",
        "  text1 = vectorizer.transform([pregunta_usuario])\n",
        "  text2 = vectorizer.transform([pregunta_dialogo])\n",
        "  return cosine_similarity(text1, text2)"
      ],
      "metadata": {
        "id": "jVCoQ4Upj01Z"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para verificar si el usuário inició un diálogo\n",
        "def dialogo(user_response):\n",
        "  user_response = tratamiento_texto(user_response) #Tratando el texto\n",
        "  user_response = re.sub(r\"[^\\w\\s]\", '', user_response) #Elimina signos de puntuación\n",
        "  consulta = \" \".join(user_response)\n",
        "  df = df_dialogo.copy()\n",
        "  for idx,row in df.iterrows():\n",
        "    df.at[idx,'interseccion'] = interseccion(user_response, row[\"dialogo\"])\n",
        "    df.at[idx,'similarity'] = similarity(consulta, row[\"dialogo\"])[0][0]\n",
        "    df.at[idx,'jaro_winkler'] = jellyfish.jaro_winkler_similarity(consulta, row[\"dialogo\"])\n",
        "    df.at[idx,'probabilidad'] = max(df.at[idx,'interseccion'],df.at[idx,'similarity'],df.at[idx,'jaro_winkler'])\n",
        "  df.sort_values(by=['probabilidad','jaro_winkler'], inplace=True, ascending=False)\n",
        "  probabilidad = df['probabilidad'].head(1).values[0]\n",
        "  if probabilidad >= 0.93:\n",
        "    print('Respuesta encontrada por el método de comparación de textos - Probabilidad: ', probabilidad)\n",
        "    respuesta = df['respuesta'].head(1).values[0]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n"
      ],
      "metadata": {
        "id": "LtfQGPd-nncQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar tu modelo entrenado aqui(recuerda siempre cargar el modelo y el vectorizer o tokenizer usado en el entrenamiento del modelo):\n",
        "ruta_modelo = '/content/drive/MyDrive/Chatbot/modelo'\n",
        "Modelo_TF = BertForSequenceClassification.from_pretrained(ruta_modelo)\n",
        "tokenizer_TF = BertTokenizer.from_pretrained(ruta_modelo)\n"
      ],
      "metadata": {
        "id": "oPaJnQHAnpsn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clase_encontrada(pregunta):\n",
        "  frase = ' '.join(normalizar(pregunta))\n",
        "    # Tokenizar la frase de entrada\n",
        "  tokens = tokenizer_TF.encode_plus(\n",
        "      frase,\n",
        "      add_special_tokens=True,\n",
        "      max_length=128,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  # Obtener los input_ids y attention_mask\n",
        "  input_ids = tokens['input_ids']\n",
        "  attention_mask = tokens['attention_mask']\n",
        "\n",
        "  # Realizar la predicción\n",
        "  with torch.no_grad():\n",
        "      outputs = Modelo_TF(input_ids, attention_mask)\n",
        "\n",
        "  # Obtener las etiquetas predichas\n",
        "  etiquetas_predichas = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "  # Decodificar las etiquetas predichas\n",
        "  etiquetas_decodificadas = etiquetas_predichas.tolist()\n",
        "  diccionario = {14: 'Sentimiento', 13: 'Saludos', 10: 'Nombre', 9: 'Identidad', 6: 'ElProfeAlejo', 1: 'Aprendizaje', 8: 'Funcion', 15: 'Usuario', 11: 'Origen', 5: 'Edad', 0: 'Agradecimiento', 3: 'Continuacion', 2: 'Contacto', 4: 'Despedida', 12: 'Otros', 7: 'Error'}\n",
        "  llave_buscada = etiquetas_decodificadas[0]\n",
        "  clase_encontrada = diccionario[llave_buscada]\n",
        "\n",
        "  return clase_encontrada\n",
        "\n",
        "def clasificacion_modelo(pregunta):\n",
        "\n",
        "  clase_encontrada1 = clase_encontrada(pregunta)\n",
        "\n",
        "  #Buscar respuesta más parecida en la clase encontrada\n",
        "  df = df_dialogo[df_dialogo['tipo'] == clase_encontrada1]\n",
        "  df.reset_index(inplace=True)\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  dialogos_num = vectorizer.fit_transform(df['dialogo'])\n",
        "  pregunta_num = vectorizer.transform([tratamiento_texto(pregunta)])\n",
        "  similarity_scores = cosine_similarity(dialogos_num, pregunta_num)\n",
        "  indice_pregunta_proxima = similarity_scores.argmax()\n",
        "\n",
        "  if max(similarity_scores)>0.5 and clase_encontrada1 not in ['Otros']:\n",
        "    #print('Respuesta encontrada por el modelo Transformers - tipo:',clase_encontrada1)\n",
        "    respuesta = df['respuesta'][indice_pregunta_proxima]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "\n",
        "  #Buscar respuesta más parecida en la clase encontrada\n",
        "  df = df_dialogo[df_dialogo['tipo'] == clase_encontrada]\n",
        "  df.reset_index(inplace=True)\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  dialogos_num = vectorizer.fit_transform(df['dialogo'])\n",
        "  pregunta_num = vectorizer.transform([tratamiento_texto(pregunta)])\n",
        "  similarity_scores = cosine_similarity(dialogos_num, pregunta_num)\n",
        "  indice_pregunta_proxima = similarity_scores.argmax()\n",
        "\n",
        "  if max(similarity_scores)>0.5 and clase_encontrada not in ['Otros']:\n",
        "    print('Respuesta encontrada por el modelo Transformers - tipo:',clase_encontrada)\n",
        "    respuesta = df['respuesta'][indice_pregunta_proxima]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "#Función para devolver la respuesta de los documentos\n",
        "def respuesta_documento(pregunta):\n",
        "  pregunta = normalizar(pregunta)\n",
        "  def contar_coincidencias(frase):\n",
        "    return sum(1 for elemento in pregunta if elemento in frase)\n",
        "\n",
        "  diccionario = {valor: posicion for posicion, valor in enumerate(lista_frases_normalizadas)}\n",
        "  lista = sorted(list(diccionario.keys()), key=contar_coincidencias, reverse=True)[:6]\n",
        "    #Hasta aqui ya tengo mi lista con las 6 respuestas con mayor coincidencia de tokens\n",
        "  #Convierte la pregunta en frase\n",
        "  #Adiciona la frase convertida al final de la lista\n",
        "  #Inicializa un TfidfVectorizer utilizando la función normalizar como tokenizer\n",
        "  #Entrenalo con fit_transform sobre la lista y asigna el resultado a la variable tfidf\n",
        "  lista.append(''.join(pregunta))\n",
        "  for elemento in lista:\n",
        "    print(elemento)\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=normalizar, token_pattern=None)\n",
        "  tfidf = TfidfVec.fit_transform(lista)\n",
        "\n",
        "  #Aplica cosine_similarity entre el último elemento de tfidf y todos los elementos de tfidf y guárdalo en vals\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  print(vals)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = round(flat[-2],2)\n",
        "  if req_tfidf>=0.22:\n",
        "    print('Respuesta encontrada por el método TfidfVectorizer - Probabilidad:', req_tfidf)\n",
        "    respuesta = lista_frases[diccionario[lista[idx]]]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "#Función para devolver una respuesta final buscada en todos los métodos disponibles\n",
        "def respuesta_chatbot(pregunta):\n",
        "  respuesta = respuesta_documento(pregunta)\n",
        "  if respuesta != '':\n",
        "    return respuesta\n",
        "  else:\n",
        "    respuesta = dialogo(pregunta)\n",
        "    if respuesta != '':\n",
        "      return respuesta\n",
        "    else:\n",
        "      respuesta = clasificacion_modelo(pregunta)\n",
        "      if respuesta != '':\n",
        "        return respuesta\n",
        "      else:\n",
        "        return 'Respuesta no encontrada'"
      ],
      "metadata": {
        "id": "Bpxajyck3pav"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta='ciencia de datos'\n",
        "respuesta_documento(pregunta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "rvCzkLKpyRuT",
        "outputId": "d9b2d103-03f0-4c65-c298-4ee4576cd8cc"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datascience ciencia dato ser campo academico interdisciplinario utilizar estadistico computacion cientifico metodo proceso algoritmo sistema obtener recolectar extraer tratar analizar presentar informe partir ruidoso estructurado no\n",
            "datascience ciencia dato ser multifacetico poder describir paradigma investigacion metodo disciplina flujo trabajo profesion\n",
            "datascience ciencia dato integrar conocimiento dominio aplicacion subyacente ejemplo economico finanza medicina natural tecnologia informacion ser concepto consistente unificar estadistico analisis informatica metodo relacionado comprender analizar fenomeno real\n",
            "datascience ciencia dato utilizar metodo tecnica teoria extraido campo dentro contexto matematica estadistica computacion informacion conocimiento dominio\n",
            "datascience ciencia dato ser diferente informatica estadistica informacion ganador premio Turing Jim Gray imaginar cuarto paradigma empirico teorico computacional ahora basado afirmar todo estar cambiar debido impacto tecnologia avalancha\n",
            "datascience ciencia dato haber resultar mucho disciplina reciente creacion realidad concepto el utilizar primero vez cientifico danes Peter Naur decada sesenta sustituto computacional\n",
            "datasciencecienciadato\n",
            "[[0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Ejecutar Chatbot"
      ],
      "metadata": {
        "id": "LMEwexpz4gdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta='hola como estas mi hermano?'\n",
        "respuesta = respuesta_chatbot(pregunta)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wq8HMP4hmU",
        "outputId": "1bd36126-8055-4faa-c1da-a340b2a77566"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datascience ciencia dato ser diferente informatica estadistica informacion ganador premio Turing Jim Gray imaginar cuarto paradigma empirico teorico computacional ahora basado afirmar todo estar cambiar debido impacto tecnologia avalancha\n",
            "investigador Yangyong Zhu Yun Xiong Research Center for Dataology and Data Science publicar Introduction to donde manifestar diferencia ciencia natural social Datologia Ciencia Datos tomar dato red objeto estudio ser lanzar IEEE Task Force on Advanced Analytics primero conferencia internacional International Conference\n",
            "datascientist datascience roadmap convertir cientifico dato existir forma adquirir conocimiento necesario universidad estar empezar ofrecer curso diplomado alguno maestria doctorado ciencia\n",
            "IBM Network Skills ofrecer Certificado Profesional Ciencia Datos tener costar el poder pedir ayuda economico curar gratuitamente estar compuesto curso duracion mes mas informacion\n",
            "ejemplo aplicacion Ciencia Datos Marketing septiembre BusinessWeek publicar articulo base dato manifestar empresa recopilar gran cantidad informacion cliente ser analizar predecir probabilidad comprar producto afirmar el utilizar conocimiento elaborar mensaje marketing calibrado precision individuo buscar conseguir asimismo explicar ochenta entusiasmo provocado propagacion lector codigo barra terminar decepcion generalizado pues abrumar lograr hacer algo util embargo creer no haber mas remedio desafinar frontera desarrollar tecnologia necesario\n",
            "datascience ejemplo aplicacion Ciencia Datos Gobernanza America Latina Banco Interamericano Desarrollo BID haber desarrollar estudio exploratorio el analizar ciencia dato implementacion diseno politica publico region tomar caso pais Argentina Brasil presentar recomendacion mantenimiento este ir tema movilidad urbano sostenible ciudad inteligente seguridad propiedad privacidad sugerencia presentado investigacion estar lograr inteligencia valor tener potencialidad ser componente estrategico toma decision evaluacion otro capacidad campo mejora rendicion contar gobierno ciudadania promover avance curaduria institucion\n",
            "holaestarhermano\n",
            "[[0. 0. 0. 0. 0. 0. 1.]]\n",
            "Estoy bien gracias Y tú\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta='inteligencia artificial?'\n",
        "respuesta = respuesta_chatbot(pregunta)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdPfvAwbqcS1",
        "outputId": "cb5b25c8-33be-4830-9e96-529283ac9615"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta clasificion de modelos\n",
            "Respuesta no encontrada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta='cuantos idiomas habla el profe alejandro?'\n",
        "respuesta = respuesta_chatbot(pregunta)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IaOQGD1q6O4",
        "outputId": "d8fdc36d-e87a-4d3f-ebeb-b6297dfa117a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta clasificion de modelos\n",
            "Respuesta no encontrada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_RbbCy0q8FI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}